{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eafcd8c",
   "metadata": {},
   "source": [
    "# Interactive Lookahead Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922b58a",
   "metadata": {},
   "source": [
    "LLMs generate text one single prediction at a time. This makes it hard for users to visualize multiple possible continuations of a token. This limits the user's ability to interact with multiple branching options, especially in creative writing. Our goal for this project was to create an interface that would let the user explore multiple 'lookahead' completions of tokens interactively. \n",
    "\n",
    "The standard Hugging Face .generate() API does not support lookahead branching directly. With .generate() API, we cannot intervene after each token to explore multiple possible next tokens. It produces and outputs one sequence at a time. And because of this reason, we needed to custom implement this inference pipeline ourselves.\n",
    "\n",
    "Lookahead generation is a technique to see explore multiple possible next-token continuations of a prompt. This will allow the user to see and choose from several potential paths instead of just a single prediction. This enables dynamic user involvement by allowing user to steer the direction of the interaction.\n",
    "\n",
    "Real world goal:\n",
    "\n",
    "Technical goal:\n",
    "\n",
    "Approach: Implementing a low level implementation of the 'generate' API to allow us to take full control over the internal process like cache, braching, and computing logits for evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5413aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.45.0-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<6,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting numpy<3,>=1.23 (from streamlit)\n",
      "  Using cached numpy-2.2.5-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting packaging<25,>=20 (from streamlit)\n",
      "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pandas<3,>=1.4.0 (from streamlit)\n",
      "  Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting pillow<12,>=7.1.0 (from streamlit)\n",
      "  Using cached pillow-11.2.1-cp313-cp313-win_amd64.whl.metadata (9.1 kB)\n",
      "Collecting protobuf<7,>=3.20 (from streamlit)\n",
      "  Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl.metadata (593 bytes)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Using cached pyarrow-20.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting requests<3,>=2.27 (from streamlit)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-extensions<5,>=4.4.0 (from streamlit)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
      "  Using cached watchdog-6.0.0-py3-none-win_amd64.whl.metadata (44 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from streamlit) (6.4.2)\n",
      "Collecting jinja2 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jsonschema>=3.0 (from altair<6,>=4.0->streamlit)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting narwhals>=1.14.2 (from altair<6,>=4.0->streamlit)\n",
      "  Downloading narwhals-1.38.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: colorama in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas<3,>=1.4.0->streamlit)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3,>=1.4.0->streamlit)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.27->streamlit)\n",
      "  Using cached certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->altair<6,>=4.0->streamlit)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.0->altair<6,>=4.0->streamlit)\n",
      "  Using cached rpds_py-0.24.0-cp313-cp313-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Using cached streamlit-1.45.0-py3-none-any.whl (9.9 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached numpy-2.2.5-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
      "Using cached pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "Using cached pillow-11.2.1-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "Using cached protobuf-6.30.2-cp310-abi3-win_amd64.whl (431 kB)\n",
      "Using cached pyarrow-20.0.0-cp313-cp313-win_amd64.whl (25.7 MB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached watchdog-6.0.0-py3-none-win_amd64.whl (79 kB)\n",
      "Using cached certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Downloading narwhals-1.38.0-py3-none-any.whl (338 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading jsonschema_specifications-2025.4.1-py3-none-any.whl (18 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Using cached referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Using cached rpds_py-0.24.0-cp313-cp313-win_amd64.whl (239 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: pytz, watchdog, urllib3, tzdata, typing-extensions, toml, tenacity, smmap, rpds-py, pyarrow, protobuf, pillow, packaging, numpy, narwhals, MarkupSafe, idna, click, charset-normalizer, certifi, cachetools, blinker, attrs, requests, referencing, pandas, jinja2, gitdb, pydeck, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "Successfully installed MarkupSafe-3.0.2 altair-5.5.0 attrs-25.3.0 blinker-1.9.0 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 click-8.1.8 gitdb-4.0.12 gitpython-3.1.44 idna-3.10 jinja2-3.1.6 jsonschema-4.23.0 jsonschema-specifications-2025.4.1 narwhals-1.38.0 numpy-2.2.5 packaging-24.2 pandas-2.2.3 pillow-11.2.1 protobuf-6.30.2 pyarrow-20.0.0 pydeck-0.9.1 pytz-2025.2 referencing-0.36.2 requests-2.32.3 rpds-py-0.24.0 smmap-5.0.2 streamlit-1.45.0 tenacity-9.1.2 toml-0.10.2 typing-extensions-4.13.2 tzdata-2025.2 urllib3-2.4.0 watchdog-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "077eab0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 14:14:18.565 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "st.set_page_config(page_title=\"Interactive Text Generator\", layout=\"centered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe223486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.7.0-cp313-cp313-win_amd64.whl.metadata (29 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.3.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\calvin semester work\\spring 2025\\ml\\gemma-lookahead\\.venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Using cached torch-2.7.0-cp313-cp313-win_amd64.whl (212.5 MB)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached fsspec-2025.3.2-py3-none-any.whl (194 kB)\n",
      "Using cached PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading setuptools-80.3.1-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 5.9 MB/s eta 0:00:00\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, tqdm, sympy, setuptools, safetensors, regex, pyyaml, networkx, fsspec, filelock, torch, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.3.2 huggingface-hub-0.30.2 mpmath-1.3.0 networkx-3.4.2 pyyaml-6.0.2 regex-2024.11.6 safetensors-0.5.3 setuptools-80.3.1 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.51.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1770cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Calvin Semester Work\\Spring 2025\\ML\\gemma-lookahead\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d435304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 14:21:18.400 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:21:18.715 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run d:\\Calvin Semester Work\\Spring 2025\\ML\\gemma-lookahead\\.venv\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-05-06 14:21:18.717 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:21:18.718 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:21:21.310 Thread 'Thread-8': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:21:21.313 Thread 'Thread-8': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:21:30.521 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:21:30.523 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "torch.classes.__path__ = [] # add this line to manually set it to empty.\n",
    "## Workaround for the issue with torch.classes.__path__ in transformers library\n",
    "## Reference: https://discuss.streamlit.io/t/message-error-about-torch/90886/6\n",
    "\n",
    "# --- 1. Setup ---\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_name = \"Alina3234/gemma-lookahead\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "model, tokenizer, device = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1409c746",
   "metadata": {},
   "source": [
    "## Lookahead Token Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac4b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookahead_sequences(model, tokenizer, hypotheses, n_branch_tokens=5, device='cuda'):\n",
    "    assert len(hypotheses.shape) == 2 and hypotheses.shape[0] == 1, \"Expected input shape (1, seq_len)\"\n",
    "    n_tokens_so_far = hypotheses.shape[1]\n",
    "    hypotheses = hypotheses.to(device)\n",
    "    past_key_values = DynamicCache()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(hypotheses, output_hidden_states=True, past_key_values=past_key_values)\n",
    "\n",
    "    branch_tokens = outputs.logits[0, -1].topk(n_branch_tokens).indices.to(device)\n",
    "    assert branch_tokens.shape == (n_branch_tokens,)\n",
    "\n",
    "    for i in range(len(past_key_values.key_cache)):\n",
    "        past_key_values.key_cache[i] = past_key_values.key_cache[i].repeat(n_branch_tokens, 1, 1, 1).to(device)\n",
    "        past_key_values.value_cache[i] = past_key_values.value_cache[i].repeat(n_branch_tokens, 1, 1, 1).to(device)\n",
    "\n",
    "    past_key_values.reorder_cache(torch.arange(n_branch_tokens, device=device))\n",
    "\n",
    "    sequences = branch_tokens.unsqueeze(1)\n",
    "    position_id = n_tokens_so_far\n",
    "    loop_output_logits = []\n",
    "\n",
    "    for step in range(2):\n",
    "        cache_position_tensor = torch.tensor([position_id], device=device)\n",
    "        attention_mask = torch.ones((n_branch_tokens, 1), dtype=torch.long, device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            current_input = sequences[:, -1:]\n",
    "            model_outs = model(\n",
    "                current_input,\n",
    "                past_key_values=past_key_values,\n",
    "                output_hidden_states=True,\n",
    "                use_cache=True,\n",
    "                cache_position=cache_position_tensor,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "        next_token_logits = model_outs.logits[:, -1]\n",
    "        next_tokens = next_token_logits.argmax(dim=-1)\n",
    "        sequences = torch.cat([sequences, next_tokens.unsqueeze(1)], dim=1)\n",
    "        loop_output_logits.append(model_outs.logits)\n",
    "        position_id += 1\n",
    "\n",
    "    return sequences, outputs.logits[0, -1], loop_output_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c49027",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lookahead_text(model, tokenizer, sequence, n_branch_tokens=5, device='cuda'):\n",
    "    sequences, _, _ = get_lookahead_sequences(model, tokenizer, sequence, n_branch_tokens, device)\n",
    "    return tokenizer.batch_decode(sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79966e",
   "metadata": {},
   "source": [
    "## Generating a lookahead sequence using the whole prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6beb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_lookahead(prompt, n_branch_tokens=5):\n",
    "    \"\"\"Generate initial lookahead with full prompt tokenization\"\"\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    return generate_lookahead_text(model, tokenizer, input_ids, n_branch_tokens, device), input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf69bda",
   "metadata": {},
   "source": [
    "## Generating a lookahead sequence based on the new token\n",
    "Need to ask Alina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739c3b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_incremental_lookahead(new_token, n_branch_tokens=5):\n",
    "    # Tokenize just the new token\n",
    "    input_ids = tokenizer(new_token, add_special_tokens=False, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    full_prompt = st.session_state.prompt\n",
    "    full_input_ids = tokenizer(full_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    return generate_lookahead_text(model, tokenizer, full_input_ids, n_branch_tokens, device), full_input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02b2e2",
   "metadata": {},
   "source": [
    "Streamlit UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f68482",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 14:22:31.412 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.413 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.414 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.415 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.415 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.416 Session state does not function when running a script without `streamlit run`\n",
      "2025-05-06 14:22:31.417 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.418 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.418 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.419 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.420 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.421 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.421 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.423 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.424 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.426 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.427 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.428 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.429 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.430 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.431 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.432 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.433 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.433 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.434 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.435 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.435 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.436 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.437 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.438 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.439 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.442 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.443 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.445 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.446 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.447 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.447 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-05-06 14:22:31.448 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "st.title(\"✍️ Interactive Lookahead Text Generator\")\n",
    "st.markdown(\"This app shows potential continuations as you write. Select a suggestion to continue your text.\")\n",
    "\n",
    "# Initialize session state variables\n",
    "if \"prompt\" not in st.session_state:\n",
    "    st.session_state.prompt = \"\"\n",
    "\n",
    "if \"suggestions\" not in st.session_state:\n",
    "    st.session_state.suggestions = []\n",
    "\n",
    "if \"last_token_added\" not in st.session_state:\n",
    "    st.session_state.last_token_added = \"\"\n",
    "\n",
    "if \"input_ids\" not in st.session_state:\n",
    "    st.session_state.input_ids = None\n",
    "\n",
    "if \"regenerate\" not in st.session_state:\n",
    "    st.session_state.regenerate = False\n",
    "\n",
    "# Function to handle suggestion selection\n",
    "def select_suggestion(suggestion):\n",
    "    # Store the original prompt length before adding the suggestion\n",
    "    original_length = len(st.session_state.prompt)\n",
    "\n",
    "    # Add the suggestion to the prompt\n",
    "    st.session_state.prompt += \" \" + suggestion\n",
    "\n",
    "    # Store the newly added token for incremental generation\n",
    "    st.session_state.last_token_added = suggestion\n",
    "\n",
    "    # Flag that we need to regenerate\n",
    "    st.session_state.regenerate = True\n",
    "\n",
    "    # Clear the current suggestions as they're no longer relevant\n",
    "    st.session_state.suggestions = []\n",
    "\n",
    "# Prompt input area\n",
    "prompt_input = st.text_area(\n",
    "    \"Your text:\",\n",
    "    value=st.session_state.prompt,\n",
    "    height=150,\n",
    "    key=\"prompt_area\"\n",
    ")\n",
    "\n",
    "# Check if the user has manually edited the prompt\n",
    "if prompt_input != st.session_state.prompt:\n",
    "    # Update the prompt and clear any cached state\n",
    "    st.session_state.prompt = prompt_input\n",
    "    st.session_state.input_ids = None\n",
    "    st.session_state.suggestions = []\n",
    "    st.session_state.last_token_added = \"\"\n",
    "\n",
    "# Generate button\n",
    "if st.button(\"Generate Completions\", type=\"primary\"):\n",
    "    if st.session_state.prompt.strip():\n",
    "        with st.spinner(\"Generating suggestions...\"):\n",
    "            try:\n",
    "                # Generate initial suggestions based on the full prompt\n",
    "                st.session_state.suggestions, st.session_state.input_ids = generate_initial_lookahead(\n",
    "                    st.session_state.prompt\n",
    "                )\n",
    "                st.session_state.regenerate = False\n",
    "            except Exception as e:\n",
    "                st.error(f\"Error during generation: {str(e)}\")\n",
    "    else:\n",
    "        st.warning(\"Please enter some text to begin.\")\n",
    "\n",
    "# Display suggestions\n",
    "if st.session_state.suggestions:\n",
    "    st.markdown(\"### ✨ Top Branching Completions:\")\n",
    "\n",
    "    # Create columns for better layout (5 suggestions per row)\n",
    "    cols = st.columns(5)\n",
    "\n",
    "    for i, suggestion in enumerate(st.session_state.suggestions):\n",
    "        col_idx = i % 5\n",
    "        with cols[col_idx]:\n",
    "            suggestion_text = suggestion.strip()\n",
    "            if st.button(f\"{suggestion_text}\", key=f\"sugg_{i}\"):\n",
    "                select_suggestion(suggestion_text)\n",
    "                st.rerun()\n",
    "\n",
    "# Auto-regenerate after selecting a suggestion\n",
    "if st.session_state.regenerate and st.session_state.prompt.strip():\n",
    "    st.session_state.regenerate = False\n",
    "\n",
    "    with st.spinner(\"Generating new suggestions...\"):\n",
    "        try:\n",
    "            # Generate suggestions based on the incremental token\n",
    "            st.session_state.suggestions, st.session_state.input_ids = generate_incremental_lookahead(\n",
    "                st.session_state.last_token_added\n",
    "            )\n",
    "            st.rerun()\n",
    "        except Exception as e:\n",
    "            st.error(f\"Error during generation: {str(e)}\")\n",
    "\n",
    "# Show text preview and stats\n",
    "if st.session_state.prompt:\n",
    "\n",
    "    # Display character and word count\n",
    "    char_count = len(st.session_state.prompt)\n",
    "    word_count = len(st.session_state.prompt.split())\n",
    "    st.caption(f\"Characters: {char_count} | Words: {word_count}\")\n",
    "\n",
    "# Add a clear button\n",
    "if st.button(\"Clear All\"):\n",
    "    st.session_state.prompt = \"\"\n",
    "    st.session_state.suggestions = []\n",
    "    st.session_state.input_ids = None\n",
    "    st.session_state.last_token_added = \"\"\n",
    "    st.rerun()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e5f31a",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2e1179",
   "metadata": {},
   "source": [
    "Cached Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67991f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookahead_sequences(model, tokenizer, hypotheses, n_branch_tokens=5, device='cuda'):\n",
    "\n",
    "  assert len(hypotheses.shape) == 2 and hypotheses.shape[0] == 1, \"Expected input shape (1, seq_len)\"\n",
    "  # stores how long the prompt is\n",
    "  n_tokens_so_far = hypotheses.shape[1]\n",
    "  hypotheses = hypotheses.to(device)\n",
    "  past_key_values = DynamicCache() # hold key/value\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(hypotheses, output_hidden_states=True, past_key_values=past_key_values)\n",
    "\n",
    "  # Get top-k tokens from last position\n",
    "  branch_tokens = outputs.logits[0, -1].topk(n_branch_tokens).indices.to(device)\n",
    "  branched_output_logits = outputs.logits[0, -1]\n",
    "  print(tokenizer.decode(branch_tokens))\n",
    "  print(\"Branch tokens shape:\", branch_tokens.shape)  # Expected: (5,)\n",
    "  assert branch_tokens.shape == (n_branch_tokens,)\n",
    "\n",
    "  # Repeat past_key_values for each branch\n",
    "  for i in range(len(past_key_values.key_cache)):\n",
    "      past_key_values.key_cache[i] = past_key_values.key_cache[i].repeat(n_branch_tokens, 1, 1, 1).to(device)\n",
    "      past_key_values.value_cache[i] = past_key_values.value_cache[i].repeat(n_branch_tokens, 1, 1, 1).to(device)\n",
    "\n",
    "  # Fixes the internal tracking\n",
    "  past_key_values.reorder_cache(torch.arange(n_branch_tokens, device=device))\n",
    "\n",
    "  # Start sequences from the branch tokens\n",
    "  sequences = branch_tokens.unsqueeze(1)\n",
    "  print(\"Initial sequences shape:\", sequences.shape)  # Expected: (5, 1)\n",
    "  assert sequences.shape == (n_branch_tokens, 1)\n",
    "\n",
    "  position_id = n_tokens_so_far\n",
    "  cached_logits = []\n",
    "\n",
    "  for step in range(2):  # Generate 2 more tokens\n",
    "      print(f\"\\n--- Step {step + 1} ---\")\n",
    "      print(\"Current sequences shape before generation:\", sequences.shape)\n",
    "\n",
    "      cache_position_tensor = torch.tensor([position_id], device=device)  # Convert to tensor\n",
    "      # Keep attention mask as is to tell the model to fully attend to each n_branch numbered tokens\n",
    "      attention_mask = torch.ones((n_branch_tokens,1), dtype=torch.long, device=device)\n",
    "      print(\"Before generation:\")\n",
    "      print(\"past_key_values key shape:\", past_key_values.key_cache[0].shape)  # Should start as (5, ..., ..., ...)\n",
    "      print(\"attention_mask shape:\", attention_mask.shape)                     # Should be (5, 1) (1,1)\n",
    "\n",
    "\n",
    "      try:\n",
    "          with torch.no_grad():\n",
    "              current_input = sequences[:, -1:]\n",
    "              print(\"Input to model (last token):\", current_input.shape)  # Expected: (5, 1)\n",
    "              assert current_input.shape == (n_branch_tokens, 1)\n",
    "\n",
    "              model_outs = model(\n",
    "                  current_input,\n",
    "                  past_key_values=past_key_values,\n",
    "                  output_hidden_states=True,\n",
    "                  use_cache=True,\n",
    "                  cache_position=cache_position_tensor, #cache_position\n",
    "                  attention_mask=attention_mask\n",
    "              )\n",
    "              print(\"model_outs type:\", type(model_outs))\n",
    "              print(\"model_outs logits shape:\", model_outs.logits.shape)\n",
    "              loop_model_logits = model_outs.logits\n",
    "              print(\"model_outs past_key_values shapes:\")\n",
    "              if hasattr(model_outs, \"past_key_values\"):\n",
    "                  if isinstance(model_outs.past_key_values, tuple) and len(model_outs.past_key_values) > 0:\n",
    "                      print(\"First layer k/v shapes:\",\n",
    "                            model_outs.past_key_values[0][0].shape,\n",
    "                            model_outs.past_key_values[0][1].shape)\n",
    "      except Exception as e:\n",
    "          print(\"Error during model forward pass:\", e)\n",
    "          raise\n",
    "\n",
    "      next_token_logits = model_outs.logits[:, -1]\n",
    "      print(next_token_logits)\n",
    "      print(\"Next token logits shape:\", next_token_logits.shape)  # Expected: (5, vocab_size)\n",
    "      assert next_token_logits.shape[0] == n_branch_tokens\n",
    "\n",
    "      next_tokens = next_token_logits.argmax(dim=-1)\n",
    "      print(\"Next tokens shape:\", next_tokens.shape)  # Expected: (5,)\n",
    "      assert next_tokens.shape == (n_branch_tokens,)\n",
    "\n",
    "      sequences = torch.cat([sequences, next_tokens.unsqueeze(1)], dim=1)\n",
    "      print(\"Updated sequences shape:\", sequences.shape)  # Should grow (5, 2), then (5, 3)\n",
    "\n",
    "      cached_logits.append(loop_model_logits)\n",
    "      position_id += 1\n",
    "\n",
    "  print(sequences)\n",
    "  return sequences, branched_output_logits, cached_logits  # Final shape: (5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7deef65",
   "metadata": {},
   "source": [
    "Step by Step Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc50de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookahead_sequences(model, tokenizer, hypotheses, n_branch_tokens=5, device='cuda'):\n",
    "  assert len(hypotheses.shape) == 2 and hypotheses.shape[0] == 1, \"Expected input shape (1, seq_len)\"\n",
    "\n",
    "  # Get the initial sequence from the input\n",
    "  original_sequence = hypotheses[0].tolist()\n",
    "  hypotheses = hypotheses.to(device)\n",
    "\n",
    "  # Get the logits for the next token without using cache\n",
    "  with torch.no_grad():\n",
    "      outputs = model(hypotheses, output_hidden_states=True)\n",
    "\n",
    "  # Get top-k tokens from last position\n",
    "  branch_tokens = outputs.logits[0, -1].topk(n_branch_tokens).indices.to(device)\n",
    "  branched_token_logit_2 = outputs.logits[0,-1]\n",
    "  print(\"Top-k branch tokens:\", tokenizer.decode(branch_tokens))\n",
    "  print(\"Branch tokens shape:\", branch_tokens.shape)  # Expected: (5,)\n",
    "  assert branch_tokens.shape == (n_branch_tokens,)\n",
    "\n",
    "  # Create initial sequences for each branch\n",
    "  all_sequences = []\n",
    "  for branch_token in branch_tokens:\n",
    "      # Each sequence starts with the original prompt + the branch token\n",
    "      sequence = original_sequence + [branch_token.item()]\n",
    "      all_sequences.append(sequence)\n",
    "\n",
    "  # Convert to tensor for easier manipulation\n",
    "  sequences = torch.tensor([all_sequences[i] for i in range(n_branch_tokens)], device=device)\n",
    "  print(\"Initial sequences shape:\", sequences.shape)  # Expected: (5, seq_len+1)\n",
    "\n",
    "  no_cache_logits = []\n",
    "  # Generate additional tokens step by step\n",
    "  for step in range(2):  # Generate 2 more tokens\n",
    "      print(f\"\\n--- Step {step + 1} ---\")\n",
    "      print(\"Current sequences shape before generation:\", sequences.shape)\n",
    "\n",
    "      next_tokens = []\n",
    "\n",
    "      # Process each sequence independently\n",
    "      for seq_idx, sequence in enumerate(sequences):\n",
    "          # Create input for model (full sequence up to now)\n",
    "          current_input = sequence.unsqueeze(0)  # Add batch dimension\n",
    "          print(f\"Sequence {seq_idx} input shape:\", current_input.shape)\n",
    "\n",
    "          try:\n",
    "              with torch.no_grad():\n",
    "                  # Forward pass without cache or position_ids\n",
    "                  model_outs = model(\n",
    "                      current_input,\n",
    "                      output_hidden_states=True,\n",
    "                      use_cache=False\n",
    "                  )\n",
    "\n",
    "                  # Get prediction for next token\n",
    "                  next_token_logits = model_outs.logits[0, -1]\n",
    "                  no_cache_logits.append(next_token_logits)\n",
    "                  print(next_token_logits)\n",
    "                  next_token = next_token_logits.argmax(dim=-1)\n",
    "                  next_tokens.append(next_token)\n",
    "\n",
    "                  print(f\"Sequence {seq_idx} next token:\", tokenizer.decode(next_token))\n",
    "\n",
    "          except Exception as e:\n",
    "              print(f\"Error processing sequence {seq_idx}:\", e)\n",
    "              raise\n",
    "\n",
    "      # Stack the next tokens\n",
    "      next_tokens = torch.stack(next_tokens)\n",
    "      print(\"Next tokens shape:\", next_tokens.shape)  # Expected: (5,)\n",
    "\n",
    "      # Add new tokens to sequences\n",
    "      sequences = torch.cat([sequences, next_tokens.unsqueeze(1)], dim=1)\n",
    "      print(\"Updated sequences shape:\", sequences.shape)\n",
    "\n",
    "  # Print the final token sequences\n",
    "  for i, seq in enumerate(sequences):\n",
    "      print(f\"Sequence {i}:\", tokenizer.decode(seq))\n",
    "\n",
    "  return sequences, branched_token_logit_2, no_cache_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a051272",
   "metadata": {},
   "source": [
    "We compared the output logits for each steps in tokenzation of cached method and no cache method. And it gave us the result where all the logits matched with the corresponding ones.\n",
    "Using one small example, we could also see that cached method is about 4 times faster in the generation process.\n",
    "CPU times: user 2.78 s, sys: 25.2 ms, total: 2.81 s Wall time: 1.49 s\n",
    "CPU times: user 11.7 s, sys: 28.3 ms, total: 11.7 s Wall time: 5.92 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671e4cbf",
   "metadata": {},
   "source": [
    "are_equal = (\n",
    "\n",
    "len(cached_logits) == len(no_cache_logits) and\n",
    "all(torch.allclose(a, b, atol=1e-4) for a, b in zip(cached_logits, no_cache_logits))\n",
    ")\n",
    "\n",
    "print(are_equal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fdc26",
   "metadata": {},
   "source": [
    "## What we learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6e87b",
   "metadata": {},
   "source": [
    "We gained a practical understanding of the tokenization process and learned how the shapes of outputs play a crucial role in ensuring correct evaluation.\n",
    "We discovered that using cached outputs significantly reduces computational load and power consumption by enabling faster generation compared to step-by-step processing.\n",
    "We also learned that running the model on a GPU can further improve speed, although performance on a CPU was still reasonably good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115e91a",
   "metadata": {},
   "source": [
    "## Future Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2c7f4",
   "metadata": {},
   "source": [
    "\n",
    "Test out multiple branched prediction for the generation of second and third token.\n",
    "Test the limit of the cached method: how many more tokens can it predict successfully?\n",
    "Implement the whole process with GPU to save more energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef4ccd",
   "metadata": {},
   "source": [
    "## Supporting Material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d813a22",
   "metadata": {},
   "source": [
    "\n",
    "This project is based on Professor Ken Arnold's initial implementation of lookahead generation.\n",
    "https://huggingface.co/spaces/CalvinU/writing-prototypes/blob/main/custom_llm_inference.py#L66\n",
    "add Codeadd Markdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
