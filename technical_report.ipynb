{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eafcd8c",
   "metadata": {},
   "source": [
    "## Interactive Lookahead Text Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922b58a",
   "metadata": {},
   "source": [
    "LLMs output generative text one single token at a time. This makes it hard for users to visualize multiple possible continuations of a token. A token being a word or a sub-word. This limits the user's ability to interact with multiple branching options, especially in creative writing. Our goal for this project was to create an interface that would let the user explore multiple 'lookahead' completions of tokens interactively. \n",
    "\n",
    "The standard Hugging Face .generate() API does not support lookahead branching directly. It is a function that abstracts the low-level implementation of token-by-token generation. With the .generate() API, we cannot intervene after each token to explore multiple possible next tokens. It produces and outputs one sequence at a time. And because of this reason, we needed to custom implement this inference pipeline ourselves for the lookahead generation.\n",
    "\n",
    "Lookahead generation is a technique to explore multiple possible next-token continuations of a prompt. This allows the user to see and choose from several potential paths instead of just a single prediction. This enables dynamic user involvement by allowing user to steer the direction of the piece they are writing.\n",
    "\n",
    "Technical Approach: Coding a low level custom implementation of the Hugging Face .generate() API to allow us to take full manual control over the internal process like caching, and branching.\n",
    "\n",
    "Technical Goal: We already had starter code for the lookahead generation logic from Professor Arnold's existing work. However, his lookahead sequence was limited to 2 next tokens for each branch. One of our main technical goals was to expand upon his existing backend logic code to support more than 2 next tokens and to evaluate and validate its accuracy. \n",
    "\n",
    "Real-World Goal: To make text generation from an LLM more collaborative, interactive and exploratory for the user.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12dc20",
   "metadata": {},
   "source": [
    "## Model Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5413aa6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1770cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minji\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.cache_utils import DynamicCache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7846b234",
   "metadata": {},
   "source": [
    "### Comparing Cached vs. Non-Cached Forward Passes Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b76c1",
   "metadata": {},
   "source": [
    "Since we are manually implementing the generation functionality, we also need to correctly handle the caching of past key-value pairs. This is important because the accuracy and efficiency of our custom API depend on proper caching.\n",
    "\n",
    "To verify that our caching logic is correct, we compare the model's output logits with and without caching. Specifically, we define two functions—one that uses caching and one that doesn't. If both functions produce the same logits for the same inputs, we can conclude that the caching has been implemented correctly.\n",
    "\n",
    "Below, you'll find both functions. They perform the same forward pass but differ in how they manage cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995288ca",
   "metadata": {},
   "source": [
    "#### Cached Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67991f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 18.1 μs\n"
     ]
    }
   ],
   "source": [
    "def get_lookahead_sequences_with_cache(model, tokenizer, hypotheses, n_branch_tokens=5, device='cuda'):\n",
    "\n",
    "  assert len(hypotheses.shape) == 2 and hypotheses.shape[0] == 1, \"Expected input shape (1, seq_len)\"\n",
    "  # stores how long the prompt is\n",
    "  n_tokens_so_far = hypotheses.shape[1]\n",
    "  hypotheses = hypotheses.to(device)\n",
    "  past_key_values = DynamicCache() # hold key/value\n",
    "\n",
    "  with torch.no_grad():\n",
    "      outputs = model(hypotheses, output_hidden_states=True, past_key_values=past_key_values)\n",
    "\n",
    "  # Get top-k tokens from last position\n",
    "  branch_tokens = outputs.logits[0, -1].topk(n_branch_tokens).indices.to(device)\n",
    "  branched_output_logits = outputs.logits[0, -1]\n",
    "  assert branch_tokens.shape == (n_branch_tokens,)\n",
    "\n",
    "  # Repeat past_key_values for each branch\n",
    "  for i in range(len(past_key_values.key_cache)):\n",
    "      past_key_values.key_cache[i] = past_key_values.key_cache[i].repeat(n_branch_tokens, 1, 1, 1).to(device)\n",
    "      past_key_values.value_cache[i] = past_key_values.value_cache[i].repeat(n_branch_tokens, 1, 1, 1).to(device)\n",
    "\n",
    "  # Fixes the internal tracking\n",
    "  past_key_values.reorder_cache(torch.arange(n_branch_tokens, device=device))\n",
    "\n",
    "  # Start sequences from the branch tokens\n",
    "  sequences = branch_tokens.unsqueeze(1) \n",
    "  assert sequences.shape == (n_branch_tokens, 1) # Expected: (5, 1)\n",
    "\n",
    "  position_id = n_tokens_so_far\n",
    "  cached_logits = []\n",
    "\n",
    "  for step in range(2):  # Generate 2 more tokens\n",
    "      \n",
    "      cache_position_tensor = torch.tensor([position_id], device=device)  # Convert to tensor\n",
    "      attention_mask = torch.ones((n_branch_tokens,1), dtype=torch.long, device=device)\n",
    "\n",
    "      try:\n",
    "          with torch.no_grad():\n",
    "              current_input = sequences[:, -1:]\n",
    "              assert current_input.shape == (n_branch_tokens, 1) # Expected: (5, 1)\n",
    "\n",
    "              model_outs = model(\n",
    "                  current_input,\n",
    "                  past_key_values=past_key_values,\n",
    "                  output_hidden_states=True,\n",
    "                  use_cache=True,\n",
    "                  cache_position=cache_position_tensor, #cache_position\n",
    "                  attention_mask=attention_mask\n",
    "              )\n",
    "\n",
    "              loop_model_logits = model_outs.logits\n",
    "            #   This block is for past key values \n",
    "            #   print(\"model_outs past_key_values shapes:\")\n",
    "            #   if hasattr(model_outs, \"past_key_values\"):\n",
    "            #       if isinstance(model_outs.past_key_values, tuple) and len(model_outs.past_key_values) > 0:\n",
    "            #           print(\"First layer k/v shapes:\",\n",
    "            #                 model_outs.past_key_values[0][0].shape,\n",
    "            #                 model_outs.past_key_values[0][1].shape)\n",
    "      except Exception as e:\n",
    "          print(\"Error during model forward pass:\", e)\n",
    "          raise\n",
    "\n",
    "      next_token_logits = model_outs.logits[:, -1]\n",
    "      assert next_token_logits.shape[0] == n_branch_tokens # Expected: (5, vocab_size)\n",
    "\n",
    "      next_tokens = next_token_logits.argmax(dim=-1)\n",
    "      assert next_tokens.shape == (n_branch_tokens,) # Expected: (5,)\n",
    "\n",
    "      sequences = torch.cat([sequences, next_tokens.unsqueeze(1)], dim=1) # Should grow (5, 2), then (5, 3) \n",
    "\n",
    "      cached_logits.append(loop_model_logits) \n",
    "      position_id += 1\n",
    "\n",
    "  return sequences, branched_output_logits, cached_logits  # Final shape: (5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7deef65",
   "metadata": {},
   "source": [
    "#### Non-Cached Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc50de51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookahead_sequences_without_cache(model, tokenizer, hypotheses, n_branch_tokens=5, device='cuda'):\n",
    "  assert len(hypotheses.shape) == 2 and hypotheses.shape[0] == 1, \"Expected input shape (1, seq_len)\"\n",
    "\n",
    "  # Get the initial sequence from the input\n",
    "  original_sequence = hypotheses[0].tolist()\n",
    "  hypotheses = hypotheses.to(device)\n",
    "\n",
    "  # Get the logits for the next token without using cache\n",
    "  with torch.no_grad():\n",
    "      outputs = model(hypotheses, output_hidden_states=True)\n",
    "\n",
    "  # Get top-k tokens from last position\n",
    "  branch_tokens = outputs.logits[0, -1].topk(n_branch_tokens).indices.to(device)\n",
    "  branched_output_logit_2 = outputs.logits[0,-1] \n",
    "  assert branch_tokens.shape == (n_branch_tokens,) # Expected: (5,)\n",
    "\n",
    "  # Create initial sequences for each branch\n",
    "  all_sequences = []\n",
    "  for branch_token in branch_tokens:\n",
    "      # Each sequence starts with the original prompt + the branch token\n",
    "      sequence = original_sequence + [branch_token.item()]\n",
    "      all_sequences.append(sequence)\n",
    "\n",
    "  # Convert to tensor for easier manipulation\n",
    "  sequences = torch.tensor([all_sequences[i] for i in range(n_branch_tokens)], device=device) # Expected: (5, seq_len+1)  \n",
    "\n",
    "  no_cache_logits = []\n",
    "  # Generate additional tokens step by step\n",
    "  for step in range(2):  # Generate 2 more tokens\n",
    "      next_tokens = []\n",
    "\n",
    "      # Process each sequence independently\n",
    "      for seq_idx, sequence in enumerate(sequences):\n",
    "          # Create input for model (full sequence up to now)\n",
    "          current_input = sequence.unsqueeze(0)  \n",
    "          try:\n",
    "              with torch.no_grad():\n",
    "                  # Forward pass without cache or position_ids\n",
    "                  model_outs = model(\n",
    "                      current_input,\n",
    "                      output_hidden_states=True,\n",
    "                      use_cache=False\n",
    "                  )\n",
    "\n",
    "                  # Get prediction for next token\n",
    "                  next_token_logits = model_outs.logits[0, -1]\n",
    "                  no_cache_logits.append(next_token_logits)\n",
    "                  next_token = next_token_logits.argmax(dim=-1)\n",
    "                  next_tokens.append(next_token)\n",
    "\n",
    "          except Exception as e:\n",
    "              print(f\"Error processing sequence {seq_idx}:\", e)\n",
    "              raise\n",
    "\n",
    "      # Stack the next tokens\n",
    "      next_tokens = torch.stack(next_tokens)\n",
    "      # Add new tokens to sequences\n",
    "      sequences = torch.cat([sequences, next_tokens.unsqueeze(1)], dim=1)\n",
    "\n",
    "  return sequences, branched_output_logit_2, no_cache_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc7d4f",
   "metadata": {},
   "source": [
    "#### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91c68c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_name = \"Alina3234/gemma-lookahead\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"After careful\"\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
    "cache_results, branched_logits, cached_logits  = get_lookahead_sequences_with_cache(model, tokenizer, input_ids, device=device)\n",
    "no_cache_results, branched_token_logit_2, no_cache_logits = get_lookahead_sequences_without_cache(model, tokenizer, input_ids, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd7b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "cached_logits_list = []\n",
    "for group in cached_logits:\n",
    "    # group has shape (5, 1, N), so we squeeze the middle dimension\n",
    "    squeezed = group.squeeze(1)  # shape becomes (5, N)\n",
    "    # then split into list of tensors\n",
    "    cached_logits_list.extend(list(squeezed))\n",
    "\n",
    "are_equal = (\n",
    "    len(cached_logits_list) == len(no_cache_logits) and\n",
    "    all(torch.allclose(a, b, atol=1e-4) for a, b in zip(cached_logits_list, no_cache_logits))\n",
    ")\n",
    "print(are_equal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a051272",
   "metadata": {},
   "source": [
    "We compared the output logits for each steps in tokenzation of cached method and no cache method. And it gave us the result where all the logits matched with the corresponding ones.\n",
    "\n",
    "Using one small example, we could also see that cached method is about 4 times faster in the generation process.\n",
    "\n",
    "CPU times: user 2.78 s, sys: 25.2 ms, total: 2.81 s Wall time: 1.49 s\n",
    "\n",
    "CPU times: user 11.7 s, sys: 28.3 ms, total: 11.7 s Wall time: 5.92 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4fdc26",
   "metadata": {},
   "source": [
    "## What we learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae6e87b",
   "metadata": {},
   "source": [
    "1. We gained a practical understanding of the tokenization process and learned how the shapes of outputs play a crucial role in ensuring correct evaluation.\n",
    "2. We discovered that using cached outputs significantly reduces computational load and power consumption by enabling faster generation compared to step-by-step processing.\n",
    "3. We also learned that running the model on a GPU can further improve speed, although performance on a CPU was still reasonably good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115e91a",
   "metadata": {},
   "source": [
    "## Future Direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2c7f4",
   "metadata": {},
   "source": [
    "\n",
    "1. Test out multiple branched prediction for the generation of second and third token.\n",
    "2. Test the limit of the cached method: how many more tokens can it predict successfully with cache?\n",
    "3. Implement the whole process with GPU to save more energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef4ccd",
   "metadata": {},
   "source": [
    "## Supporting Material"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d813a22",
   "metadata": {},
   "source": [
    "\n",
    "This project is based on Professor Ken Arnold's initial implementation of lookahead generation.\n",
    "https://huggingface.co/spaces/CalvinU/writing-prototypes/blob/main/custom_llm_inference.py#L66\n",
    "add Codeadd Markdown"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
